{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2fce09-170b-43e2-8419-cf2ddc6a1b3b",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd4dab-58a5-411d-b5e1-0ccd03a206c8",
   "metadata": {},
   "source": [
    "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9c1ac-cb62-4315-8d8e-d40e8390b5d7",
   "metadata": {},
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57763ecb-3b34-4480-897f-4c0b0a6432ac",
   "metadata": {},
   "source": [
    "You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\n",
    "\n",
    "* Early stopping :- \n",
    "Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.\n",
    "* Pruning :- \n",
    "You might identify several features or parameters that impact the final prediction when you build a model. Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones. For example, to predict if an image is an animal or human, you can look at various input parameters like face shape, ear position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\n",
    "* Regularization :- \n",
    "Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. For example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\n",
    "* Ensembling :-\n",
    "Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while bagging trains them in parallel.\n",
    "* Data augmentation :-\n",
    "Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics. For example, applying transformations such as translation, flipping, and rotation to input images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ea295-0734-4048-b250-6b6847edfcc8",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c6ac4-0bfc-4f95-8b83-a4c210cfc9d7",
   "metadata": {},
   "source": [
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data.\n",
    "\n",
    "Underfitting occurs when a learning model oversimplifies the data in the set. The results in underfit models show low variance and high bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d556f62-95d3-4475-a928-f7d1f8e42ac6",
   "metadata": {},
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f724353-27c1-4b1d-a17d-96bb9e2dc74d",
   "metadata": {},
   "source": [
    "* Bias Variance Tradeoff\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4c416-5623-4e9f-9ae0-07d9232b1c30",
   "metadata": {},
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3cb57d-de60-48ec-aa69-bc04f7ac4959",
   "metadata": {},
   "source": [
    "The best method to detect overfit models is by testing the machine learning models on more data with with comprehensive representation of possible input data values and types. Typically, part of the training data is used as test data to check for overfitting. A high error rate in the testing data indicates overfitting. One method of testing for overfitting is given below.\n",
    "K-fold cross-validation\n",
    "Cross-validation is one of the testing methods used in practice. In this method, data scientists divide the training set into K equally sized subsets or sample sets called folds. The training process consists of a series of iterations. During each iteration, the steps are:\n",
    "1.    Keep one subset as the validation data and train the machine learning model on the remaining K-1 subsets.\n",
    "2.    Observe how the model performs on the validation sample.\n",
    "3.    Score model performance based on output data quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cb242-e99a-412c-bc5e-ef473cf89160",
   "metadata": {},
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0d052-4f90-44be-85d8-3a436a5ccc12",
   "metadata": {},
   "source": [
    "Any supervised machine learning algorithm should strive to achieve low bias and low variance as its primary objectives. This scenario, however, is not feasible for two reasons: first, bias and variance are negatively related to one another; and second, it is extremely unlikely that a machine learning model could have both a low bias and a low variance at the same time.\n",
    "\n",
    "In contrast to bias, variance describes the situation in which the model accounts for the variations in the data as well as the noise. If you try to change the algorithm so that it is more suitable for a certain dataset, it may end up having a low bias, but the variance will increase.\n",
    "\n",
    "Characteristics of a high bias model include:\n",
    "\n",
    "* Failure to capture proper data trends\n",
    "* Potential towards underfitting\n",
    "* More generalized/overly simplified\n",
    "* High error rate\n",
    "\n",
    "Characteristics of a high variance model include:\n",
    "\n",
    "* Noise in the data set\n",
    "* Potential towards overfitting\n",
    "* Complex models\n",
    "* Trying to put all data points as close as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d70445-0830-49ee-a235-8466200b18da",
   "metadata": {},
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b060da-b45c-47ee-93e6-121b0df27cc6",
   "metadata": {},
   "source": [
    "Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. The commonly used regularization techniques are : \n",
    "\n",
    "Lasso Regularization – L1 Regularization\n",
    "Ridge Regularization – L2 Regularization\n",
    "Elastic Net Regularization – L1 and L2 Regularization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
